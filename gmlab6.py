# -*- coding: utf-8 -*-
"""GMlab6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N2RHNPw5AWE9iWTStCz3gAcsAnbU_v0K
"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline

import warnings
warnings.filterwarnings('ignore')

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics import accuracy_score, f1_score

categories = [
    'comp.graphics',
    'sci.space',
]
dataset = fetch_20newsgroups(subset='train', categories=categories)
dir(dataset)

print(dataset.data[1])

print(dataset.target[:10])

docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=123)

"""Preprocessing
---
We use TfidfVectorizer from sklearn.feature_extraction.text to convert each of the documents in docs_train into a Bag-of-Words vector (with idf modification). The docs_train collection will be a matrix. 

Bag-of-Words Representation
---
- Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).
- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of attribute/feature #j where j is the index of word w in the dictionary.

The bags of words representation implies that # of attributes is the number of distinct words in the corpus: this number is typically very large.

To deal with the problem that longer documents have higher average count values than shorter documents, we divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf (*Term Frequencies*).

We also downscale weights for words that occur in many documents in the dataset/corpus and are therefore less informative than those that occur only in a smaller portion of the dataset/corpus. The result is tf-idf (*Term Frequency times Inverse Document Frequency*)
"""

vect = TfidfVectorizer(min_df=3, max_df=0.95)
dm = vect.fit_transform(docs_train)
dm.shape

def to_numpy(v):
    return np.array(v.todense())[0]

d1 = to_numpy(dm[0])
plot(d1)
print(len(np.nonzero(d1)[0]))

"""Task 1 
---
Build a pipeline that inlcude a TfidfVectorizer and a SVC classifier (use kernel='linear'). Train the model and use it to make predictions on the test data. Calculate the accuracy score of the model on the test data.
"""

tfidf = TfidfVectorizer(min_df=3, max_df=0.95)
svc = SVC(kernel='linear')
pipeline = Pipeline([('vect', tfidf), ('svc', svc)])

pipeline.fit(docs_train, y_train)
y_predicted = pipeline.predict(docs_test)
print(accuracy_score(y_test, y_predicted))

"""Task 2
---
Using GridSearchCV, do a search to find the best values for: 
 - parameter 'ngram_range' of the TfidfVectorizer, try (1, 1) and (1, 2) 
 - parameter 'C' of SVC, try 1000 and 10000 
 
Print the best hyper parameter values.
"""

pipeline = Pipeline([('vect', tfidf), ('svc', svc)])
params = {'svc__C':[1000, 10000], 'vect__ngram_range':[(1, 1), (1, 2)]}

grid_search = GridSearchCV(pipeline, params, cv=5)
grid_search.fit(docs_train, y_train)

print(grid_search.best_params_)

y_predicted = grid_search.predict(docs_test)
print(accuracy_score(y_test, y_predicted))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_predicted, target_names=dataset.target_names))

"""Use the best model found by the grid search to make predictions on the test data. Print the accuracy score."""