# -*- coding: utf-8 -*-
"""GeraldMoutonHW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xj39_EREQiWXF6e1f1CHvmwv4Lc8FNgL

Homework 2
---
**Due Nov. 15 (Thu) by end of the day.**

Do all your work on this notebook. Submit your homework by uploading the notebook file to moodle. Your submission notebook should contain:

- Code
- Output from running your code (printouts)

***
"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import load_files
from sklearn.model_selection import train_test_split

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, f1_score

"""### The movie review dataset"""

import cloudpickle as cp
from urllib.request import urlopen
dataset = cp.load(urlopen("https://drive.google.com/uc?export=download&id=1tqjekAEy_SM_sJUvjIBbajp6I-_WmHgj"))

docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=123)

"""Task 1 
---
Use TfidfVectorizer to fit_transform the training data (docs_train) and then transform the test data (docs_test)
"""

from sklearn.feature_extraction.text import TfidfVectorizer

vect = TfidfVectorizer(min_df=3, max_df=0.95)
X_train = vect.fit_transform(docs_train)
X_test = vect.transform(docs_test)

"""Task 2 
---
Build a SVC model to predict whether a movie review is positive or negative. Test the model accuracy on the test data. Try different values (0.01, 0.1, 1, 10) for the hyper parameter 'C' and print out the model accuracy for each of the parameter value.
"""

from sklearn.svm import SVC

for c in [0.01, 0.1, 1, 10]:
    m = SVC(kernel='linear', C=c)
    m.fit(X_train, y_train)
    y_predicted = m.predict(X_test)
    print(accuracy_score(y_test, y_predicted))

"""Task 3
---
Naive Bayes is a prediction model based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.

Build a Naive Bayes model to predict whether a movie review is positive or negative. Test the model accuracy on the test data.
"""

from sklearn.naive_bayes import MultinomialNB

m = MultinomialNB()
m.fit(X_train, y_train)
y_predicted = m.predict(X_test)
print(accuracy_score(y_test, y_predicted))

"""Task 4
---
A random forest is a ensemble (collective) model that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.

Build a random forest model to predict whether a movie review is positive or negative. Test the model accuracy on the test data. Try different values (20, 100, 500) for the hyper parameter 'n_estimators', i.e., the number of decision trees in the ensemble, and print out the model accuracy for each of the parameter value.
"""

from sklearn.ensemble import RandomForestClassifier

for e in [20, 100, 500]:
    m = RandomForestClassifier(n_estimators=e)
    m.fit(X_train, y_train)
    y_predicted = m.predict(X_test)
    print(accuracy_score(y_test, y_predicted))

"""#### From the above tasks, you can observe that different models and different choice of hyper-parameter values can lead to quite different prediction performance. What is the model (and hyper-parameter) among the above that gives the best prediction? What gives the worst?

SVM with C=1 gives the best performance and SVM with C=0.01 gives the worst.
"""